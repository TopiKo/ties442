Phase 3 of the project:

Enviroment state is completely determined by the history of the game. In the history however, there can be multiple attributes: the choises of the opponent and the agent, time taken from the opponent to perform each choice, the actions the agent used to distract the opponent etc. Clearly the action history of the agend is defined by the agent but also the action history of the opponent is at least somewhat effected by the agents actions. The state of the enviroment evolves as the amount of history increases. We have to have somekind of method of deciding how far past history is still relevant at the current moment, also should we consider 'general' history of all played games ever?

The history is constantly evolving as the game advances. Each action of the agent and opponent are appended to the history. This history determines the enviroment for our agent. From the few possiple choices that the agent can take it tries to find the one which is the most likely to win the next round. This can then also mean trying to distract the opponent by some means. As a state diagram this can be viewed as a simple loop that always return to the phase of analyzing the enviroment i.e., the history of the game, each time the history has evolved and we have new possibilities to learn from it. 

The 'goal state' for the agent at each round is of course be victorious, unless the agent decides to choose stradegy where it loses first and the wins (hopefully) twice or similar. However, I could imagine that such stradegies are quite impossible in the game under consideration. As there is huge(!) amount of uncertainty involved in the evolution of the enviroment it is difficult to view the problem as a search from some initial state to another. Of course if the enviroment would be behaving very consistently then finding the 'goal' state is trivial.  
